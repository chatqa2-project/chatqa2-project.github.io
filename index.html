<html>
  <head>
    <meta charvctk_set="UTF-8">
    <title>Introducing ChatQA-2</title>
    <style type="text/css">

      /* css for table */
      .tg  {border-collapse:collapse;border-color:#9ABAD9;border-spacing:0;}
      .tg td{background-color:#EBF5FF;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#444;
        font-family:Arial, sans-serif;font-size:12px;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tg th{background-color:#409cff;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#fff;
        font-family:Arial, sans-serif;font-size:12px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tg .tg-center{text-align:center;vertical-align:top}
      .tg .tg-left{text-align:left;vertical-align:top}

      /* css for table */
      .tt  {border-collapse:collapse;border-color:#9ABAD9;border-spacing:0;}
      .tt td{background-color:#EBF5FF;border-color:#9ABAD9;border-style:solid;border-width:0px;color:#444;
        font-family:Arial, sans-serif;font-size:13px;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tt th{background-color:#409cff;border-color:#9ABAD9;border-style:solid;border-width:0px;color:#fff;
        font-family:Arial, sans-serif;font-size:13px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tt .tt-center{text-align:center;vertical-align:top}
      .tt .tt-left{text-align:left;vertical-align:top}
      .tt .tt-center_boardertop{text-align:center;vertical-align:top;border-top-width:1.6px}
      .tt .tt-left_boardertop{text-align:left;vertical-align:top;border-top-width:1.6px}

      /* css for text container */
      .container {
        margin: 0 auto; /* Center the container horizontally */
        max-width: 900px; /* Set the maximum width of the container */
        text-align: left; /* Left-align the text within the container */
        padding: 0 20px; /* Optional: Add padding to the container */
        line-height: 1.5; /* Adjust line height for spacing between lines */
        font-size: 18px; /* Set the font size for the container */
      }
      
      /* color for subtitle */
      .colored_subtitle {
        color: rgb(91, 91, 91); /* Set the color of the text */
      }

      .custom-link {
            color: #76b900; /* Sets the color to blue */
            text-decoration: none; /* Removes underline */
      }

      .img-chatrag {
        max-width: 92%; /* Set the maximum width of the image to 100% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */
      }

      .img-full {
        max-width: 100%; /* Set the maximum width of the image to 100% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */
      }

      .img-humaneval {
        max-width: 55%; /* Set the maximum width of the image to 100% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */
      }

      pre {
        background-color: #f4f4f4;
        padding: 10px;
        border-radius: 5px;
        font-size: 15px;
        overflow-x: auto;
      }
      code {
        display: block;
      }
      
    </style>
  </head>


  <div class="container">
    <br>
    <h2>Introducing ChatQA-2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities</h2>
    <h4> <a href="https://huggingface.co/nvidia/" class="custom-link">Model WeightsðŸ¤—</a> &ensp; <a href="https://huggingface.co/datasets/nvidia/" class="custom-link">Evaluation DataðŸ¤—</a> &ensp; <a href="https://huggingface.co/datasets/nvidia/" class="custom-link">Training DataðŸ¤—</a> &ensp; <a href="https://huggingface.co/intfloat/e5-mistral-7b-instruct" class="custom-link">RetrieverðŸ¤—</a> &ensp; <a href="https://arxiv.org/abs/2407.14482" class="custom-link">Paper</a>  </h4>
    <p>
    Today (September 9th, 2024), we release ChatQA-2, which achieves better accuracy than GPT-4-Turbo-2024-04-09 on many long-context understanding tasks and surpasses it on the RAG benchmark <a href="https://arxiv.org/abs/2407.14482" class="custom-link">ChatQA-2 paper</a>, and it is built on the top of the <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" class="custom-link">Llama-3 base model</a>. Specifically, we incorporate more long context pretraining data as well as long SFT data. We open-source the model weights, training data, evaluation data, and SFT recipe for future study.
    </p>

    <h3>Overview</h3>
    <!-- <h4 class="colored_subtitle">Evaluation on </h4> -->
    <figure>
     <img class="img-full" src="figs/overview.png">
    </figure>
    We compare our ChatQA-2 long context model with the state-of-the-art long context LLMs across tasks with varying context lengths. Note taht, we open-source the training data and reproduction recipe for building long-context LLMs from short-context base LLMs}, resources that are not currently available for open-access long context models like Qwen2 and Llama3.1. Our model achieves the best average score on 4 real-world ultra-long context (beyond 100K) tasks from InfiniteBench, and 9 short context (within 4K) tasks from \textsc{ChatRAG Bench}.   
    <br>
    
    <h3>Needle In A Haystack</h3>
    <!-- <h4 class="colored_subtitle">Evaluation on </h4> -->
    <figure>
     <img class="img-full" src="figs/llama3_long_128k_370.png">
    </figure>
   <br>

   We use the needle test: "The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day". We
find this is generally a harder pressure test than the "passkey" in a haystack test, which is commonly
used by other long-context LLM works. It shows that our model achieves 100% accuracy confirming our modelâ€™s perfect long-context retrieval capability.

    <h3>Tasks beyond 100k</h3>
    <figure>
      <img class="img-chatrag" src="figs/100k_eval.png">
    </figure>
    We evaluate four realworld dowstream tasks from InfiniteBench. Our model (41.04) outperforms many existing state-of-the-art models, such as GPT4-Turbo-2024-04-09 (33.16), GPT4-1106 preview (28.23),
    Llama3.1-70B-Instruct (39.81), Qwen2-72B-Instruct (39.77), Llama-3-70B-Instruct-Gradient-262k
    (32.57) and Claude 2 (33.96), showing a state-of-the-art long-context capability of our model.
    <br>

  <h3>Tasks within 32k</h3>
     <!-- <h4 class="colored_subtitle">Evaluation on </h4> -->
     <figure>
      <img class="img-chatrag" src="figs/32k_eval.png">
     </figure>
    <br>
  
  GPT-4-Turbo-2024-04-09 achieves the highest score of 51.93 among all models. Our
  model scores 48.15, which is higher than Llama-3-70B-Instruct-Gradient-262k but is slightly lower than
  Qwen2-72B-Instruct and Llama3.1-70B-Instruct. Additionally,
  we found that all the RAG solutions perform worse than the long context solution, which suggest all
  these SOTA long context LLMs can really handle 32K tokens within their context window.

  <h3>Tasks winthin 4k</h3>
    We reuse <a href="https://huggingface.co/datasets/nvidia/ChatRAG-Bench" class="custom-link">ChatRAG Bench</a>: a benchmark for evaluating a model's conversational QA capability over documents or retrieved context within 4k context window.
    ChatRAG Bench consisting of 10 datasets, which covers a wide range of documents and question types, which require models to generate responses from long context, comprehend and reason over tables, conduct arithmetic calculations, and indicate when questions cannot be found within the context. We report the score without HybriDialogue.

    <figure>
      <img class="img-full" src="figs/4k_eval.png">
    </figure>
    Our model achieves an average score of 56.30, outperforming most existing SOTA models with 128K context window, including
GPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct. It is also comparable,
though slightly behind, the Llama3-ChatQA-1.5-70B (57.14) with only 4K context length.
This suggests that extending short-context models to long-context is not a free lunch.
    <br>

    <h3>RAG vs Long Context</h3>
    <figure>
      <img class="img-chatrag" src="figs/RAG_more_samples.png">
    </figure>
    For downstream tasks within 32k sequence length, we found that more tokens
    consistently yield better results. Also,
    RAG can still outperform the long-context solution if a sufficient number of top-k chunks are used, i.e., at least 12,000 tokens as shown in Figure
    2. Therefore, the hyperparameter k can be selected based on the tradeoff between efficiency and
    accuracy in real-world applications.
    <br>

    <figure>
      <img class="img-chatrag" src="figs/RAG_vs_long_context_100k_plus.png">
    </figure>

    On the other hand, for context lengths beyond 100K, RAG (using top-30 chunks for
    our Llama3-ChatQA-2-70B, and top-20 for Qwen2-72B-Instruct) outperforms the full long-context
    solution. In such scenarios, RAG is recommended for better accuracy and much
    lower inference cost, provided it is applicable to the downstream tasks.
    
    <br>

  <!-- <h3>License</h3>
    The use of the ChatQA-1.5-8B and ChatQA-1.5-70B are governed by the <a href="https://llama.meta.com/llama3/license/" class="custom-link">META LLAMA 3 COMMUNITY LICENSE AGREEMENT</a>. -->
    
  <!-- <h3>Citation</h3>
  <pre><code>@article{liu2024chatqa,
  title={ChatQA: Surpassing GPT-4 on Conversational QA and RAG},
  author={Liu, Zihan and Ping, Wei and Roy, Rajarshi and Xu, Peng and Lee, Chankyu and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2401.10225},
  year={2024}}</code></pre> -->

  </div>
  
  <br><br><br>

</html>


