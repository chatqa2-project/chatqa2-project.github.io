<html>
  <head>
    <meta charvctk_set="UTF-8">
    <title>Introducing ChatQA-2</title>
    <style type="text/css">

      /* css for table */
      .tg  {border-collapse:collapse;border-color:#9ABAD9;border-spacing:0;}
      .tg td{background-color:#EBF5FF;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#444;
        font-family:Arial, sans-serif;font-size:12px;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tg th{background-color:#409cff;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#fff;
        font-family:Arial, sans-serif;font-size:12px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tg .tg-center{text-align:center;vertical-align:top}
      .tg .tg-left{text-align:left;vertical-align:top}

      /* css for table */
      .tt  {border-collapse:collapse;border-color:#9ABAD9;border-spacing:0;}
      .tt td{background-color:#EBF5FF;border-color:#9ABAD9;border-style:solid;border-width:0px;color:#444;
        font-family:Arial, sans-serif;font-size:13px;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tt th{background-color:#409cff;border-color:#9ABAD9;border-style:solid;border-width:0px;color:#fff;
        font-family:Arial, sans-serif;font-size:13px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tt .tt-center{text-align:center;vertical-align:top}
      .tt .tt-left{text-align:left;vertical-align:top}
      .tt .tt-center_boardertop{text-align:center;vertical-align:top;border-top-width:1.6px}
      .tt .tt-left_boardertop{text-align:left;vertical-align:top;border-top-width:1.6px}

      /* css for text container */
      .container {
        margin: 0 auto; /* Center the container horizontally */
        max-width: 900px; /* Set the maximum width of the container */
        text-align: left; /* Left-align the text within the container */
        padding: 0 20px; /* Optional: Add padding to the container */
        line-height: 1.5; /* Adjust line height for spacing between lines */
        font-size: 18px; /* Set the font size for the container */
      }
      
      /* color for subtitle */
      .colored_subtitle {
        color: rgb(91, 91, 91); /* Set the color of the text */
      }

      .custom-link {
            color: #76b900; /* Sets the color to blue */
            text-decoration: none; /* Removes underline */
      }

      .img-chatrag {
        max-width: 92%; /* Set the maximum width of the image to 100% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */

      }

      .img-70 {
        max-width: 70%; /* Set the maximum width of the image to 100% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */

      }

      .img-75 {
        max-width: 75%; /* Set the maximum width of the image to 100% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */

      }

      .img-fit {
        max-width: 80%; /* Set the maximum width of the image to 80% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */

      }

      .img-full {
        max-width: 100%; /* Set the maximum width of the image to 100% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */

      }

      .img-60 {
        max-width: 60%; /* Set the maximum width of the image to 100% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */
      }

      pre {
        background-color: #f4f4f4;
        padding: 10px;
        border-radius: 5px;
        font-size: 15px;
        overflow-x: auto;
      }
      .flex-content{
        display: flex;
        align-items: center;
        justify-content: center;
      }
      code {
        display: block;
      }
      
    </style>
  </head>


  <div class="container">
    <br>
    <h2>ChatQA-2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities</h2>
    <h4> <a href="https://huggingface.co/nvidia/" class="custom-link">Model Weightsü§ó</a> &ensp; <a href="https://huggingface.co/datasets/nvidia/" class="custom-link">Evaluation Dataü§ó</a> &ensp; <a href="https://huggingface.co/datasets/nvidia/" class="custom-link">Training Dataü§ó</a> &ensp; <a href="https://arxiv.org/abs/2407.14482" class="custom-link">Paper</a>  </h4>
    <p>
    Today (September 9th, 2024), we release <a href="https://arxiv.org/abs/2407.14482" class="custom-link">ChatQA-2</a>, which achieves higher accuracy than GPT-4-Turbo-2024-04-09, Llama-3.1-70B-Instruct, and Qwen2-72B-Instruct on 128K long-context understanding tasks and surpasses them on the RAG benchmark. It was built on the top of the <a href="https://huggingface.co/meta-llama/Meta-Llama-3-70B" class="custom-link">Llama-3 base model</a> with 8K context window. We open-source the model weights, training data, evaluation data, and SFT recipe for future study.
    </p>

    <h3>Overview</h3>
    <!-- <h4 class="colored_subtitle">Evaluation on </h4> -->
    The long-context capability of LLMs is sometimes considered a rival technique to retrieval-augmented generation (RAG).
    In fact, <strong>RAG and long-context techniques complement each other from a pragmatic perspective</strong>.
    An LLM with a long context window can either process large volumes of text as a prompt or utilize retrieval methods to efficiently extract relevant information from the extensive text, depending on the downstream tasks and accuracy vs. efficiency trade-offs.
    RAG has efficiency advantages and can easily retrieve relevant contexts for query-based tasks (e.g, QA) from billions of tokens, a feat that long context models cannot achieve so far. Meanwhile, long context models are good at tasks such as summarizing entire documents, where RAG can not perform. 
    As a result, <strong>the state-of-the-art LLM needs to excel at both capabilities, providing options for different downstream tasks and meeting different accuracy-efficiency trade-off requirements</strong>.
    <figure>
      <div class="flex-content" style="text-align: center;">
        <img class="img-fit" src="figs/overview.png">
     </div>
    </figure>
    We compare our ChatQA-2 long-context model with state-of-the-art long-context LLMs across tasks of varying context lengths. Notably, we open-source the training data and reproduction recipe for building 128K long-context LLMs from pretrained base models with 8K context, resources that are not currently available for open-access (open-weights) 128K context models like Qwen2 and Llama3.1. Our model achieves the highest average score on four real-world ultra-long context tasks (beyond 100K) from InfiniteBench, as well as nine short-context tasks (within 4K) from ChatRAG Bench.
    <br>
    
    <h3>Needle In A Haystack</h3>
    <!-- <h4 class="colored_subtitle">Evaluation on </h4> -->
    <figure>
    <div class="flex-content" style="text-align: center;">
     <img class="img-60" src="figs/llama3_1_instruct_70b_tp8_mcore.png">
     </div>
     <div class="flex-content" style="text-align: center;">
     <img class="img-60" src="figs/chatqa2_70b_paul.png">
     </div>
    </figure>
   We evaluate our model and Llama3.1-70B-Instruct on the Needle In A
   Haystack (NIAH) test which is popular for testing the long-context
   capability of LLMs, and can be considered as a threshold level evaluation. We use the needle test:
   ‚ÄúThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.‚Äù We
   find this is generally a harder pressure test than the ‚Äúpasskey‚Äù in a haystack test, which is also commonly
   used by other long-context LLM works. The figure above shows that our model can pass the NIAH test with 100% accuracy where Llama3.1-Instruct-70B fails at sequence length around 128k, although both of them can pass ‚Äúpasskey‚Äù in a haystack test.
   <br>

<h3>RAG vs Long Context</h3>
<figure>
  <div class="flex-content" style="text-align: center;">
  <img class="img-60" src="figs/RAG_more_samples.png"></div>
</figure>
This figure shows the ablation of our model with RAG given different top-<i>k</i> = {5, 10, 20, 40} retrieval, and  chunk-size = {300, 600, 1200} on long context benchmarks within 32K tokens.
The accuracy can be monotonically improved with more retrieved tokens (i.e., <i>k</i> &times; chunk_size) in the context window.
Interestingly, RAG can still outperform the long-context solution if a sufficient number of top-<i>k</i> chunks or input tokens are used, i.e., at least 12,000 tokens. Therefore, the hyperparameter <i>k</i> can be selected based on the tradeoff between efficiency and
accuracy in real-world applications.
<br>



<h3>Tasks beyond 100k</h3>
<figure>
  <div class="flex-content" style="text-align: center;">
  <img class="img-70" src="figs/100k_eval.png">
</div>
</figure>
We evaluate four realworld dowstream tasks from InfiniteBench. Our model (41.04) outperforms many existing state-of-the-art models, such as GPT4-Turbo-2024-04-09 (33.16), GPT4-1106 preview (28.23),
Llama3.1-70B-Instruct (39.81), Qwen2-72B-Instruct (39.77), Llama-3-70B-Instruct-Gradient-262k
(32.57) and Claude 2 (33.96), showing a state-of-the-art long-context capability of our model.
<br>

<h3>Tasks within 32k</h3>
  <!-- <h4 class="colored_subtitle">Evaluation on </h4> -->
  <figure>
    <div class="flex-content" style="text-align: center;">
  <img class="img-75" src="figs/32k_eval.png">
</div>
  </figure>
GPT-4-Turbo-2024-04-09 achieves the highest score of 51.93 among all models. Our
model scores 48.15, which is higher than Llama-3-70B-Instruct-Gradient-262k but is slightly lower than
Qwen2-72B-Instruct and Llama3.1-70B-Instruct. Additionally,
we found that long context solution is better than a RAG solution using top-<i>5</i> with the chunk size as 1200, which suggest all
these SOTA long context LLMs can really handle 32K tokens within their context window.
<br>

<h3>Tasks winthin 4k</h3>
We reuse <a href="https://huggingface.co/datasets/nvidia/ChatRAG-Bench" class="custom-link">ChatRAG Bench</a>: a benchmark for evaluating a model's conversational QA capability over documents or retrieved context within 4k context window.
ChatRAG Bench consisting of 10 datasets, which covers a wide range of documents and question types, which require models to generate responses from long context, comprehend and reason over tables, conduct arithmetic calculations, and indicate when questions cannot be found within the context. We report the score without HybriDialogue.

<figure>
  <div class="flex-content" style="text-align: center;">
  <img class="img-full" src="figs/4k_eval.png"></div>
</figure>
Our model achieves an average score of 56.30, outperforming most existing SOTA models with 128K context window, including
GPT-4-Turbo-2024-04-09 (54.72), Qwen2-72B-Instruct (54.06), and Llama3.1-70B-Instruct (52.12). It is also comparable,
though slightly behind, the Llama3-ChatQA-1.5-70B (57.14) which is our best model with 4K context length.
<br>

  <!-- <h3>License</h3>
    The use of the ChatQA-1.5-8B and ChatQA-1.5-70B are governed by the <a href="https://llama.meta.com/llama3/license/" class="custom-link">META LLAMA 3 COMMUNITY LICENSE AGREEMENT</a>. -->
    
  <!-- <h3>Citation</h3>
  <pre><code>@article{liu2024chatqa,
  title={ChatQA: Surpassing GPT-4 on Conversational QA and RAG},
  author={Liu, Zihan and Ping, Wei and Roy, Rajarshi and Xu, Peng and Lee, Chankyu and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2401.10225},
  year={2024}}</code></pre> -->

  </div>
  
  <br><br><br>

</html>


