<html>
  <head>
    <meta charvctk_set="UTF-8">
    <title>Introducing ChatQA-2</title>
    <style type="text/css">

      /* css for table */
      .tg  {border-collapse:collapse;border-color:#9ABAD9;border-spacing:0;}
      .tg td{background-color:#EBF5FF;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#444;
        font-family:Arial, sans-serif;font-size:12px;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tg th{background-color:#409cff;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#fff;
        font-family:Arial, sans-serif;font-size:12px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tg .tg-center{text-align:center;vertical-align:top}
      .tg .tg-left{text-align:left;vertical-align:top}

      /* css for table */
      .tt  {border-collapse:collapse;border-color:#9ABAD9;border-spacing:0;}
      .tt td{background-color:#EBF5FF;border-color:#9ABAD9;border-style:solid;border-width:0px;color:#444;
        font-family:Arial, sans-serif;font-size:13px;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tt th{background-color:#409cff;border-color:#9ABAD9;border-style:solid;border-width:0px;color:#fff;
        font-family:Arial, sans-serif;font-size:13px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tt .tt-center{text-align:center;vertical-align:top}
      .tt .tt-left{text-align:left;vertical-align:top}
      .tt .tt-center_boardertop{text-align:center;vertical-align:top;border-top-width:1.6px}
      .tt .tt-left_boardertop{text-align:left;vertical-align:top;border-top-width:1.6px}

      /* css for text container */
      .container {
        margin: 0 auto; /* Center the container horizontally */
        max-width: 900px; /* Set the maximum width of the container */
        text-align: left; /* Left-align the text within the container */
        padding: 0 20px; /* Optional: Add padding to the container */
        line-height: 1.5; /* Adjust line height for spacing between lines */
        font-size: 18px; /* Set the font size for the container */
      }
      
      /* color for subtitle */
      .colored_subtitle {
        color: rgb(91, 91, 91); /* Set the color of the text */
      }

      .custom-link {
            color: #76b900; /* Sets the color to blue */
            text-decoration: none; /* Removes underline */
      }

      .img-chatrag {
        max-width: 92%; /* Set the maximum width of the image to 100% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */
      }

      .img-full {
        max-width: 100%; /* Set the maximum width of the image to 100% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */
      }

      .img-humaneval {
        max-width: 55%; /* Set the maximum width of the image to 100% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */
      }

      pre {
        background-color: #f4f4f4;
        padding: 10px;
        border-radius: 5px;
        font-size: 15px;
        overflow-x: auto;
      }
      code {
        display: block;
      }
      
    </style>
  </head>


  <div class="container">
    <br>
    <h2>Introducing ChatQA-2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities</h2>
    <h4> <a href="https://huggingface.co/nvidia/" class="custom-link">Model Weightsü§ó</a> &ensp; <a href="https://huggingface.co/datasets/nvidia/" class="custom-link">Evaluation Dataü§ó</a> &ensp; <a href="https://huggingface.co/datasets/nvidia/" class="custom-link">Training Dataü§ó</a> &ensp; <a href="https://huggingface.co/intfloat/e5-mistral-7b-instruct" class="custom-link">Retrieverü§ó</a> &ensp; <a href="https://arxiv.org/abs/2407.14482" class="custom-link">Paper</a>  </h4>
    <p>
    Today (September 9th, 2024), we release ChatQA-2, which achieves better accuracy than GPT-4-Turbo-2024-04-09 on many long-context understanding tasks and surpasses it on the RAG benchmark <a href="https://arxiv.org/abs/2407.14482" class="custom-link">ChatQA-2 paper</a>, and it is built on the top of the <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" class="custom-link">Llama-3 base model</a>. Specifically, we incorporate more long context pretraining data as well as long SFT data. We open-source the model weights, training data, evaluation data, and SFT recipe for future study.
    </p>

    <h3>Overview</h3>
    <!-- <h4 class="colored_subtitle">Evaluation on </h4> -->
    <figure>
     <img class="img-full" src="figs/overview.png">
    </figure>
    We compare our ChatQA-2 long context model with the state-of-the-art long context LLMs across tasks with varying context lengths. Note that, we open-source the training data and reproduction recipe for building long-context LLMs from short-context base LLMs, resources that are not currently available for open-access long context models like Qwen2 and Llama3.1. Our model achieves the best average score on 4 real-world ultra-long context (beyond 100K) tasks from InfiniteBench, and 9 short context (within 4K) tasks from ChatRAG Bench.   
    <br>
    
    <h3>Needle In A Haystack</h3>
    <!-- <h4 class="colored_subtitle">Evaluation on </h4> -->
    <figure>
     <img class="img-full" src="figs/llama3_1_instruct_70b_tp8_mcore.png">
     <img class="img-full" src="figs/chatqa2_70b_paul.png">
    </figure>
   <br>
   We evaluate our model and Llama3.1-70B-Instruct on the Needle In A
   Haystack (NIAH) test which is popular for testing the long-context
   capability of LLMs, and can be considered as a threshold level evaluation. We use the needle test:
   ‚ÄúThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.‚Äù We
   find this is generally a harder pressure test than the ‚Äúpasskey‚Äù in a haystack test, which is also commonly
   used by other long-context LLM works. The figure above shows that our model can pass the NIAH test with 100% accuracy where Llama3.1-Instruct-70B fails at sequence length around 128k.  

<h3>RAG vs Long Context</h3>
<figure>
  <img class="img-chatrag" src="figs/RAG_more_samples.png">
</figure>
This figure shows the ablation of our model with RAG given different top-<i>k</i> = {5, 10, 20, 40} retrieval, and  chunk-size = {300, 600, 1200} on long context benchmarks within 32K tokens.
The accuracy can be monotonically improved with more retrieved tokens (i.e., <i>k</i> &times; chunk_size) in the context window.
Interestingly, RAG can still outperform the long-context solution if a sufficient number of top-<i>k</i> chunks or input tokens are used, i.e., at least 12,000 tokens. Therefore, the hyperparameter <i>k</i> can be selected based on the tradeoff between efficiency and
accuracy in real-world applications.
<br>

<h3>Tasks beyond 100k</h3>
<figure>
  <img class="img-chatrag" src="figs/100k_eval.png">
</figure>
We evaluate four realworld dowstream tasks from InfiniteBench. Our model (41.04) outperforms many existing state-of-the-art models, such as GPT4-Turbo-2024-04-09 (33.16), GPT4-1106 preview (28.23),
Llama3.1-70B-Instruct (39.81), Qwen2-72B-Instruct (39.77), Llama-3-70B-Instruct-Gradient-262k
(32.57) and Claude 2 (33.96), showing a state-of-the-art long-context capability of our model.
<br>

<h3>Tasks within 32k</h3>
  <!-- <h4 class="colored_subtitle">Evaluation on </h4> -->
  <figure>
  <img class="img-chatrag" src="figs/32k_eval.png">
  </figure>
<br>

GPT-4-Turbo-2024-04-09 achieves the highest score of 51.93 among all models. Our
model scores 48.15, which is higher than Llama-3-70B-Instruct-Gradient-262k but is slightly lower than
Qwen2-72B-Instruct and Llama3.1-70B-Instruct. Additionally,
we found that long context solution is better than a RAG solution using top-<i>5</i> with the chunk size as 1200, which suggest all
these SOTA long context LLMs can really handle 32K tokens within their context window.

<h3>Tasks winthin 4k</h3>
We reuse <a href="https://huggingface.co/datasets/nvidia/ChatRAG-Bench" class="custom-link">ChatRAG Bench</a>: a benchmark for evaluating a model's conversational QA capability over documents or retrieved context within 4k context window.
ChatRAG Bench consisting of 10 datasets, which covers a wide range of documents and question types, which require models to generate responses from long context, comprehend and reason over tables, conduct arithmetic calculations, and indicate when questions cannot be found within the context. We report the score without HybriDialogue.

<figure>
  <img class="img-full" src="figs/4k_eval.png">
</figure>
Our model achieves an average score of 56.30, outperforming most existing SOTA models with 128K context window, including
GPT-4-Turbo-2024-04-09 (54.72), Qwen2-72B-Instruct (54.06), and Llama3.1-70B-Instruct (52.12). It is also comparable,
though slightly behind, the Llama3-ChatQA-1.5-70B (57.14) which is our best model with 4K context length.
This suggests that our long context extension maintains the short context capability abd extending short-context models to long-context is not a free lunch.
<br>

  <!-- <h3>License</h3>
    The use of the ChatQA-1.5-8B and ChatQA-1.5-70B are governed by the <a href="https://llama.meta.com/llama3/license/" class="custom-link">META LLAMA 3 COMMUNITY LICENSE AGREEMENT</a>. -->
    
  <!-- <h3>Citation</h3>
  <pre><code>@article{liu2024chatqa,
  title={ChatQA: Surpassing GPT-4 on Conversational QA and RAG},
  author={Liu, Zihan and Ping, Wei and Roy, Rajarshi and Xu, Peng and Lee, Chankyu and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2401.10225},
  year={2024}}</code></pre> -->

  </div>
  
  <br><br><br>

</html>


